
# DBE task
**_Fork this repo to get started_**

As a Database Engineer, you have been tasked with optimizing the performance of an e-commerce database.

### Setup: 
Create a PostgreSQL database using docker-compose and init with the supplied config/init.sql file.

### Tasks:
1. Benchmark some common queries.
      
      The queries where tested in three ways: explain analyze, timing with psql and setting **log_duration=on** and **log_min_duration_statement=0** to check what the logs registered.
   The info in the log files was obtained by following the logs in the service:
   ```
   docker compose logs -f db
   ```
   - Retrieve the names of customers who have placed orders in the last 30 days.

     The query used was:
     ``` sql
     select c.first_name , c.last_name , o.order_date  from orders o, customers c 
     where o.customer_id = c.customer_id 
     and order_date >= current_date - 30
     order by o.order_date;
     ```
     Below the duration registered by each mentioned method:

     **Explain Analyze**
     ```
     Sort  (cost=856.87..871.37 rows=5803 width=240) (actual time=50.056..53.386 rows=20000 loops=1)
        Sort Key: o.order_date
        Sort Method: quicksort  Memory: 2019kB
        ->  Hash Join  (cost=46.02..494.10 rows=5803 width=240) (actual time=4.001..35.396 rows=20000 loops=1)
              Hash Cond: (o.customer_id = c.customer_id)
              ->  Seq Scan on orders o  (cost=0.00..432.64 rows=5803 width=8) (actual time=0.049..16.223 rows=20000 loops=1)
                    Filter: (order_date >= (CURRENT_DATE - 30))
              ->  Hash  (cost=42.12..42.12 rows=312 width=240) (actual time=3.937..3.938 rows=3000 loops=1)
                    Buckets: 4096 (originally 1024)  Batches: 1 (originally 1)  Memory Usage: 196kB
                    ->  Seq Scan on customers c  (cost=0.00..42.12 rows=312 width=240) (actual time=0.037..2.215 rows=3000 loops=1)
     Planning Time: 1.170 ms
     Execution Time: 55.623 ms
     ```

     **Timing**
     ```
     contoso=# \timing
     Timing is on.
     contoso=# select c.first_name , c.last_name , o.order_date  from orders o, customers c 
     contoso-# where o.customer_id = c.customer_id 
     contoso-# and order_date >= current_date - 30
     contoso-# order by o.order_date;
      first_name | last_name | order_date 
      ------------+-----------+------------
       First1714  | Last1714  | 2023-06-28
       First279   | Last279   | 2023-06-28
       First1803  | Last1803  | 2023-06-28
       First691   | Last691   | 2023-06-28
       First1603  | Last1603  | 2023-06-28
       First1609  | Last1609  | 2023-06-28
       First384   | Last384   | 2023-06-28
       First1796  | Last1796  | 2023-06-28
       First2255  | Last2255  | 2023-06-28
       First2914  | Last2914  | 2023-06-28
       First2668  | Last2668  | 2023-06-28
       First2706  | Last2706  | 2023-06-28
       First987   | Last987   | 2023-06-28
     Time: 74.651 ms
     contoso=# 
     ```
     **Log file**
     ```
     dbe-task-db-1  | 2023-07-28 05:23:11.446 UTC [72] LOG:  duration: 49.069 ms  execute <unnamed>: select c.first_name , c.last_name , o.order_date  from orders o, customers c 
     dbe-task-db-1  |        where o.customer_id = c.customer_id 
     dbe-task-db-1  |        and order_date >= current_date - 30
     dbe-task-db-1  |        order by o.order_date
     ```

     
   - Calculate the total revenue generated by the e-commerce shop for a specific date range.

     The query used was:
     ``` sql
     select sum(total_amount) revenue from orders
     where order_date between '2023-07-01' and '2023-07-10';
     ```
      **Explain Analyze**
     ```
     Aggregate  (cost=389.34..389.35 rows=1 width=32) (actual time=11.968..11.969 rows=1 loops=1)
        ->  Seq Scan on orders  (cost=0.00..389.12 rows=87 width=16) (actual time=0.021..9.131 rows=6664 loops=1)
              Filter: ((order_date >= '2023-07-01'::date) AND (order_date <= '2023-07-10'::date))
              Rows Removed by Filter: 13336
     Planning Time: 0.133 ms
     Execution Time: 11.989 ms
     ```

     **Timing**
     ```
     contoso=# \timing
     Timing is on.
     contoso=# select sum(total_amount) revenue from orders
     contoso-# where order_date between '2023-07-01' and '2023-07-10';
       revenue   
     ------------
      3325955.87
      (1 row)

     Time: 13.392 ms
     contoso=# \timing
     Timing is off.
     contoso=# 
     ```

     **Log File**
     ```
     dbe-task-db-1  | 2023-07-28 05:23:29.743 UTC [72] LOG:  duration: 10.663 ms  execute <unnamed>: select sum(total_amount) revenue from orders
     dbe-task-db-1  |        where order_date between '2023-07-01' and '2023-07-10'
     ```
          
   - Find the top-selling products based on the quantity sold.

     The query used was:
     ``` sql
     select oi.product_id, p.product_name, sum(oi.quantity) total_quantity  from order_items oi, products p 
     where oi.product_id = p.product_id
     group by oi.product_id, p.product_name
     having sum(oi.quantity) > 1
     order by 3 desc
     ```
      **Explain Analyze**
     ```
     Sort  (cost=2376.57..2394.91 rows=7333 width=230) (actual time=148.320..148.461 rows=1000 loops=1)
       Sort Key: (sum(oi.quantity)) DESC
       Sort Method: quicksort  Memory: 87kB
       ->  HashAggregate  (cost=1630.79..1905.79 rows=7333 width=230) (actual time=146.790..147.572 rows=1000 loops=1)
             Group Key: oi.product_id, p.product_name
             Filter: (sum(oi.quantity) > 1)
             Batches: 1  Memory Usage: 913kB
             ->  Hash Join  (cost=13.47..1360.47 rows=36043 width=226) (actual time=1.166..89.863 rows=70105 loops=1)
                   Hash Cond: (oi.product_id = p.product_id)
                   ->  Seq Scan on order_items oi  (cost=0.00..1171.32 rows=65532 width=8) (actual time=0.044..30.376 rows=70105 loops=1)
                   ->  Hash  (cost=12.10..12.10 rows=110 width=222) (actual time=1.107..1.108 rows=1000 loops=1)
                         Buckets: 1024  Batches: 1  Memory Usage: 55kB
                         ->  Seq Scan on products p  (cost=0.00..12.10 rows=110 width=222) (actual time=0.031..0.635 rows=1000 loops=1)
     Planning Time: 0.476 ms
     Execution Time: 149.050 ms
     ```

     **Timing**
     ```
     contoso=# \timing
     Timing is on.
     contoso=# select oi.product_id, p.product_name, sum(oi.quantity) total_quantity  from order_items oi, products p 
     contoso-# where oi.product_id = p.product_id
     contoso-# group by oi.product_id, p.product_name
     contoso-# having sum(oi.quantity) > 1
     contoso-# order by 3 desc;
      product_id | product_name | total_quantity 
     ------------+--------------+----------------
             610 | Product610   |           1861
             999 | Product999   |           1836
              86 | Product86    |           1773
             789 | Product789   |           1324
             615 | Product615   |           1275
             938 | Product938   |           1180
             251 | Product251   |           1178
             911 | Product911   |           1132
             108 | Product108   |           1114
             760 | Product760   |           1101
               1 | Product1     |           1091
             519 | Product519   |           1071
             279 | Product279   |           1066
     Time: 115.952 ms
     contoso=#
     ```

     **Log File**
     ```
     dbe-task-db-1  | 2023-07-28 05:23:57.085 UTC [72] LOG:  duration: 128.447 ms  execute <unnamed>: select oi.product_id, p.product_name, sum(oi.quantity) total_quantity  from order_items oi, products p 
     dbe-task-db-1  |        where oi.product_id = p.product_id
     dbe-task-db-1  |        group by oi.product_id, p.product_name
     dbe-task-db-1  |        having sum(oi.quantity) > 1
     dbe-task-db-1  |        order by 3 desc
     ```

2. Create some roles and permissions
   - Create a user for "bob" and grant read only permissions.
    ``` sql
    create user bob with login password 'bob';
   grant select on all tables in schema public to bob;
   ```
   - Create a user for "dave" and grant read write permissions.
    ``` sql
    create user dave with login password 'dave';
    grant insert, select, update, delete on all tables in schema public to dave;
   ```
     _Some checks were added during the execution of this task with Ansible to validate if the users exist before creating them._
3. Review the database schema and implement any improvements or optimizations, you could make for better performance and scalability.

   - Index created on column shop_id in **customers** table due to the frequence of this value
   - Index created on columns order_id, product_id and shop_id on table **order_items**, as a means of quickly obtaining info about the orders besides the table primary key.
   - Index created on column order_date of table **orders**, to aid in the search of orders by time range.
   - In order to keep consistency of the data, some foreign keys were added; in the table **orders** from the customer_id with a reference to this column in the table **customers**, and from table **order_items** from the order_id column to the **orders** table to the same column, the same from the product_id column to the **products** table to the matching column there.

	These scripts are included in the scripts/indexes_and_constraints.sql file.

4. Identify the tables that can benefit from partitioning based on their characteristics and usage patterns.

   Out of the tables in the schema, the table customers and and orders were chosen for partition, the reasoning being that these two tables will expect the biggest increase in usability and request of info (new customers will be added/created and new orders will be made), so they suit the benefits of partitioning by segregating their data. The products table could have been another candidate, but we saw it as a "catalog" table that wasn't going to have the well defined patterns or growth for a partition.

5. Design a partitioning strategy for the selected tables, considering factors such as data distribution, query patterns, and data growth. Document and implement the partitioning strategy.

	The partition of customers and orders was done BY RANGE, based on the increments of their respective primary keys. The strategy used was the creation of a new partitioned table with our desired ranges,  inserting the data from the original tables into these new ones, and renaming the old ones. It's worth mentioning that, in a large environment. this method might present some challenges or unwanted downtime, and might not suit the needs of certain organizations.

	These scripts are located in the scripts/customers_partition.sql and scripts/orders_partition.sql.

### Bonus:
Automate all of the above using your favorite tooling, don't forget to document the process.

This implementation was automated using Ansible, and can be executed as follows:
```
ansible-playbook setup.yml
```
_In case of getting an error related to pip and docker compose_, proceed to install the respective module with pip and pass the appropiate python executable to the ansible_python_interpreter variable
```
pip3 install docker-compose
ansible-playbook setup.yml -e 'ansible_python_interpreter=/usr/bin/python3'
```

Run log
```
[dbadmin@etcd-server-1 dbe-task]$ ansible-playbook setup.yml -e 'ansible_python_interpreter=/usr/bin/python3'
[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all'

PLAY [localhost] ***********************************************************************************************************************************************************************************************************************************

TASK [Deploy and init PostgreSQL Database] *********************************************************************************************************************************************************************************************************
changed: [localhost]

TASK [Wait until database is available] ************************************************************************************************************************************************************************************************************
Pausing for 120 seconds
(ctrl+C then 'C' = continue early, ctrl+C then 'A' = abort)
ok: [localhost]

TASK [Create users and grant permissions] **********************************************************************************************************************************************************************************************************
changed: [localhost]

TASK [Partitioning customers table] ****************************************************************************************************************************************************************************************************************
changed: [localhost]

TASK [Partitioning orders table] *******************************************************************************************************************************************************************************************************************
changed: [localhost]

TASK [Creating indexes and constraints to tables] **************************************************************************************************************************************************************************************************
changed: [localhost]

PLAY RECAP *****************************************************************************************************************************************************************************************************************************************
localhost                  : ok=6    changed=5    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

[dbadmin@etcd-server-1 dbe-task]$ 
```
```
[dbadmin@etcd-server-1 dbe-task]$ docker compose ps -a
NAME                IMAGE               COMMAND                  SERVICE             CREATED             STATUS              PORTS
dbe-task_db_1       postgres:15         "docker-entrypoint.s…"   db                  3 minutes ago       Up 3 minutes        0.0.0.0:5432->5432/tcp, :::5432->5432/tcp
[dbadmin@etcd-server-1 dbe-task]$ 
```
```
[dbadmin@etcd-server-1 dbe-task]$ psql -d contoso -U usr_commerce -h localhost
Password for user usr_commerce: 
psql (14.7, server 15.3 (Debian 15.3-1.pgdg120+1))
WARNING: psql major version 14, server major version 15.
         Some psql features might not work.
Type "help" for help.

contoso=# \d
                          List of relations
 Schema |           Name           |       Type        |    Owner     
--------+--------------------------+-------------------+--------------
 public | customers                | partitioned table | usr_commerce
 public | customers_old            | table             | usr_commerce
 public | customers_p_1000_to_2000 | table             | usr_commerce
 public | customers_p_1_to_1000    | table             | usr_commerce
 public | customers_p_2000_to_3000 | table             | usr_commerce
 public | customers_p_default      | table             | usr_commerce
 public | order_items              | table             | usr_commerce
 public | orders                   | partitioned table | usr_commerce
 public | orders_old               | table             | usr_commerce
 public | orders_p_10001_to_15000  | table             | usr_commerce
 public | orders_p_15001_to_20000  | table             | usr_commerce
 public | orders_p_1_to_5000       | table             | usr_commerce
 public | orders_p_5001_to_10000   | table             | usr_commerce
 public | orders_p_default         | table             | usr_commerce
 public | products                 | table             | usr_commerce
(15 rows)

contoso=# \du
                                     List of roles
  Role name   |                         Attributes                         | Member of 
--------------+------------------------------------------------------------+-----------
 bob          |                                                            | {}
 dave         |                                                            | {}
 usr_commerce | Superuser, Create role, Create DB, Replication, Bypass RLS | {}

contoso=# 
```
